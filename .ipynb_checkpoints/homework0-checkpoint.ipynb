{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Homework #0</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def C(n, m):\n",
    "    return math.factorial(n) / (math.factorial(m) * math.factorial(n - m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) (combinatorics)  \n",
    "**Base case**:  \n",
    "$C(0,0) = 1 = {\\large \\frac{0!}{0!0!}}, C(1,0) = 1 = {\\large \\frac{1!}{0!0!}}, C(1,1) = 1 = {\\large \\frac{1!}{1!1!}}$  \n",
    "**Inductive step**:  \n",
    "Assume $C(n_t, k_t) = {\\large \\frac {n_t\\!}{k_t!(n_t - k_t)!}}$ holds for $n_t <= n, k_t <= n, k_t <= n_t$,  \n",
    "For $C(n+1, k)$,   \n",
    "If $k = n+1$, $C(n + 1,k) = 1 = {\\large \\frac{(n + 1)!}{(n + 1)!0!}}$  \n",
    "else  \n",
    "$$\\begin{eqnarray} C(n + 1, k) &=& C(n, k) + C(n, k - 1) \\\\\n",
    "             &=& \\frac {n!}{k!(n - k)!} + \\frac {n!}{(k - 1) !(n + 1 - k)!} \\\\ \n",
    "             &=& \\frac {(n + 1)!(n + 1 -k)}{k!(n + 1 - k)!(n + 1)}+ \\frac {(n + 1)!k}{k!(n + 1 - k)!(n + 1)} \\\\\n",
    "             &=& \\frac {(n + 1)!}{k!(n + 1 - k)!}\n",
    "             \\end{eqnarray} $$  \n",
    "So $C(n_t, k_t) = {\\large \\frac {n_t\\!}{k_t!(n_t - k_t)!}}$ holds for $n_t <= n + 1, k_t <= n + 1, k_t <= n_t$  \n",
    "qed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) (counting)  \n",
    "problem 1: Let P(X) be the probability of getting exactly X heads when flipping 10 fair coins. \n",
    "$$P(X = 4) = \\frac {C(10, 4)}{2^{10}} \\approx 0.205$$  \n",
    "\n",
    "problem 2: The total number of combinations of a full house(XXXYY) is:  \n",
    "$$H = C(13, 1) \\times C(4,3) \\times C(12, 1) \\times C(4, 2) = 3744$$\n",
    "The probility of getting a full house by drawing 5 cards from a deck of 52 cards if:  \n",
    "$$P(house) = \\frac{3744}{C(52, 5)} \\approx 0.00144$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) (conditional probability)  \n",
    "Let $X_1 X_2 X_3$ indicate what kind of coin my friend get after each flipping, then  \n",
    "$$\\begin{eqnarray} P(X_1 = h \\bigcap X_2 = h \\bigcap X_3 = h | X_1 = h \\bigcup X_2 = h \\bigcup X_3 = h)  \n",
    "&=& \\frac {P(X_1 = h \\bigcap X_2 = h \\bigcap X_3 = h \\bigcap (X_1 = h \\bigcup X_2 = h \\bigcup X_3 = h))}{P(X_1 = h \\bigcup X_2 = h \\bigcup X_3 = h)} \\\\ &=& \\frac {P(X_1 = h \\bigcap X_2 = h \\bigcap X_3 = h )}{P(X_1 = h \\bigcup X_2 = h \\bigcup X_3 = h)} \\\\ &=& {\\large\\frac {\\frac {1}{8}}{\\frac {7}{8}}} \\\\ &=& \\frac {1}{7}\\end{eqnarray}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) (Bayes theorem)  \n",
    "If we get an X from the program with |X| = 1, what is the probability that X is negative?\n",
    "$$P(X = -1) = P(X = -1 | X \\, in \\,\\{0, -1, -2, -3\\}) \\times P(X \\,in\\, \\{0, -1, -2, -3\\}) = \\frac{1}{4} \\times \\frac{1}{2} = \\frac{1}{8}$$\n",
    "$$P(X = 1) = P(X = 1 | X \\, in \\,\\{0, 1, ...., 7\\}) \\times P(X \\,in\\, \\{0, 1, ...., 7\\}) = \\frac{1}{8} \\times \\frac{1}{2} = \\frac{1}{16}$$\n",
    "$$P(X = 1 ∪ X = -1) = P(X = 1) + P(X = -1) = \\frac {3}{16}$$\n",
    "\n",
    "$$P(X = -1 | (X = 1 ∪ X = -1)) = \\frac {P(X = -1)}{P(X = 1 ∪ X = -1)} = \\frac{2}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) (union/intersection)  \n",
    "$P(A) = 0.3$, $P(B) = 0.4$  \n",
    "maximum possible value of $P(A ∩ B) = 0.3$   \n",
    "minimum possible value of $P(A ∩ B) = 0$  \n",
    "maximum possible value of $P(A ∪ B) = 0.7$  \n",
    "minimum possible value of $P(A ∪ B) = 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) (mean/variance)  \n",
    "proof:  \n",
    "$\\begin{eqnarray} \\sigma ^ 2 _ X &=& \\frac{1}{N - 1} \\sum_{n = 1}^{N} (X_n - \\bar X)^2 \\\\              &=& \\frac{N}{N - 1}(\\frac{1}{N}\\sum_{n=1}^NX_n^2 - \\frac{2\\bar X}{N}\\sum_{n=1}^NX_n + \\bar X^2) \\\\                                  &=& \\frac{N}{N - 1}(\\frac{1}{N}\\sum_{n=1}^NX_n^2 - 2\\bar X^2 + \\bar X^2)\\\\ \n",
    "                                 &=& \\frac{N}{N - 1}(\\frac{1}{N}\\sum_{n=1}^NX_n^2 - \\bar X^2) \\\\\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) (Gaussian distribution)  \n",
    "proof:   \n",
    "Let $f_Z(z)$ be the probability density of Z, then $f_Z(z) = \\int_{-\\infty}^{+\\infty}f_{X1}(x)f_{X2}(z - x)dx$\n",
    "$\\begin{eqnarray} f_Z(z) &=& \\int_{-\\infty}^{+\\infty}f_{X1}(x)f_{X2}(z - x)dx                 \\\\&=& \\int_{-\\infty}^{+\\infty}\\frac{1}{\\sqrt{2 \\pi} \\sigma _{X2}} exp\\left [ -\\frac{{(z - x - \\mu_{X2})}^2}{2\\sigma_{X2}^2}\\right ] \\frac{1}{\\sqrt{2 \\pi} \\sigma _{X1}} exp\\left [ -\\frac{{(x - \\mu_{X1})}^2}{2\\sigma_{X1}^2}\\right ]dx\\end{eqnarray}$  \n",
    "Let $\\sigma_Z = \\sqrt {\\sigma_{X1}^2 + \\sigma_{X2}^2} $, then  \n",
    "$\\begin{eqnarray} f_Z(z) &=& \\frac{1}{\\sqrt{2 \\pi} \\sigma _{Z}} exp\\left [ -\\frac{{(z - (\\mu_{X1} + \\mu_{X2}))}^2}{2\\sigma_{Z}^2}\\right ]\\end{eqnarray}$   \n",
    "So p(Z) is Gaussian and it's mean is -1 and it's variance is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) (rank)  \n",
    "The rank of $\\left(\\begin{matrix} 1 & 2 & 1\\\\ 1 & 0 & 3\\\\ 1 & 1 & 2 \\end{matrix} \\right)$ is the same as $\\left(\\begin{matrix} 1 & 1 & 2\\\\ 0 & 1 & -1\\\\ 0 & 0 & 0 \\end{matrix} \\right)$ which is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) (inverse)  \n",
    "The inverse of $\\left(\\begin{matrix} 0 & 2 & 4\\\\ 2 & 4 & 2\\\\ 3 & 3 & 1 \\end{matrix} \\right)$ is $\\left(\\begin{matrix} 0.125 & -0.625 & 0.75\\\\ -0.25 & 0.75 & -0.5\\\\ 0.375 & -0.375 & 0.25 \\end{matrix} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) (eigenvalues/eigenvectors)  \n",
    "The eigenvalues of $\\left(\\begin{matrix} 3 & 1 & 1\\\\ 2 & 4 & 2\\\\ -1 & -1 & 1 \\end{matrix} \\right)$ are   \n",
    "4, 2 and 2  \n",
    "The corresponding engenvectors are   \n",
    "$[0.4082,0.8165,-0.4082]^T, [0.5854,0.2003,-0.7856]^T, [0.4082,-0.8165,0.4082]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) (singular value decomposition)  \n",
    "problem 1: $MM^†M = U\\sum V^T V\\sum ^†U^T U\\sum V^T = U\\sum \\sum ^† \\sum V^T = U\\sum V^T = M$  \n",
    "\n",
    "problem 2: $MM^†M =M$ -> $M^†M =I$ -> $M^†=M^ {-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) (PD/PSD)  \n",
    "problem 1: for any $x$, let $Z^Tx = y$, $x^TZZ^Tx = y^Ty >= 0$, equals holds when $x$ is 0 vector.  \n",
    "\n",
    "problem 2: if A is symmetric, $A = P^TBP$ where P is invertible matrix and B is diagonal matrix whose diagonal elements are A's eigenvalues. For any x, let Px = y, We have:\n",
    "$$x^TAx = (Px)^TB(Px) = y^TBy = \\sum{\\lambda_i y_i^2} > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) (inner product)  \n",
    "Consider $x ∈ R^d$ and some $u ∈ R^d$ with $||u|| = 1$.  \n",
    "The maximum value of $u^Tx$ is $||x||$ when $u$ and $x$ are parallel and in the same direction.  \n",
    "The minimum value of $u^Tx$ is $-||x||$ when $u$ and $x$ are parallel and not in the same direction.  \n",
    "The minimum value of $||u^Tx||$ is $0$ when $u$ and $x$ are perpendicular.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) (distance)  \n",
    "The distance between $H_1$ and $H_2$ is 3 + 2 = 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) (differential and partial differential)  \n",
    "\n",
    "${\\large \\frac{df(x)}{dx} = \\frac{-2e^{-2x}} {1 + e^{-2x}}}$ \n",
    "\n",
    "${\\large  \\frac{\\partial g(x,y)}{\\partial y}} = e^2y + 6xye^{3xy^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) (chain rule)  \n",
    "${\\large\\frac {\\partial f}{\\partial v}} = -sin(u - v)sin(u + v) - cos(u + v)cos(u - v) = -cos(2v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) (integral)  \n",
    "$\\int_5^{10} \\frac {2}{x - 3}dx = 2ln(x - 3)|_5^{10} = 2ln(3.5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) (gradient and Hessian)  \n",
    "$\\nabla E = [{\\large \\frac {\\partial E}{\\partial u}},{\\large \\frac {\\partial E}{\\partial v}}] = [2(ue^v - 2ve^{-u})(e^v + 2ve^{-u}), 2(ue^v - 2ve^{-u})(ue^v - 2e^{-u})]$  \n",
    "$\\quad \\quad \\quad \\quad \\quad \\quad \\, \\, \\,= [2(ue^{2v} + 2uve^{v - u} -2ve^{v-u} + 4v^2e^{-2u}),2(u^2e^{2v} -2ue^{v-u} - 2uve^{v-u} + 4ve^{-2u}]$   \n",
    "\n",
    "$\\quad \\,\\,\\,= [2(e^2 + \\frac{4}{e^2}), 2(e^2 - 4 + \\frac{4}{e^2})]$  \n",
    "\n",
    "$\\begin{eqnarray} \\nabla ^2 E &=& \\left [ \\begin{matrix}{\\large \\frac {\\partial^2 E}{\\partial u^2}} & {\\large \\frac {\\partial^2 E}{\\partial u \\partial v}} \\\\ {\\large \\frac {\\partial^2 E}{\\partial v \\partial u }} & {\\large \\frac {\\partial^2 E}{\\partial v^2}} \\end{matrix} \\right ] \\\\&=& \\left [ \\begin {matrix} 2(e^2v + 4ve^{v - u} -2uve^{v - u} - 8v^2e^{-2u}) & 2(ue^{2v} + 2ue^{v - u} + 2uve^{v - u} - 2e^{v - u} - 2ve^{v - u} + 8ve^{-2u}) \\\\ 2(2ue^{2v} - 2e^{v - u} + 2ue^{v - u} - 2ve^{v-u} + 2uve^{v-u} - 8ve^{-2u})  & 2(2u^2e^{2v} -2ue^{v - u} - 2ue^{v - u} - 2uve^{v - u} + 4e^{-2u})\\\\ \\end {matrix} \\right ] \\\\ &=& \\left [ \\begin{matrix} 2(e^2 + 2 - \\frac {8}{e^2})  & 2(e^2 + \\frac {8}{e^2})\\\\ 2(2e^2 - \\frac{8}{e^2}) & 2(2e^2 - 6 + \\frac{4}{e^2}) \\end{matrix} \\right ] \\end{eqnarray}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) (Taylor's expansion)  \n",
    "$\\begin{eqnarray} E(u,v) = E(1,1) &+& (u - 1)E_u^′(1, 1) \\\\ &+& (v - 1)E_v^′(1, 1) \\\\ &+& \\frac{1}{2!}(u - 1)^2E_{uu}^{″}(1,1) \\\\ &+& \\frac{1}{2!}(u - 1)(v - 1)E_{uv}^{″}(1,1) \\\\ &+& \\frac{1}{2!}(u - 1)(v - 1)E_{vu}^{″}(1,1) \\\\ &+& \\frac{1}{2!}(v - 1)^2E_{vv}^{″}(1,1) + o^n\\end{eqnarray}$   \n",
    "\n",
    "Then just plug in the values which are calculated in (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) (optimization)  \n",
    "Let $f(\\alpha) = Ae^{\\alpha} + Be^{-2\\alpha}$  \n",
    "$f^′(\\alpha) = Ae^{\\alpha} - 2Be^{-2\\alpha}$  \n",
    "$f^″(\\alpha) = Ae^{\\alpha} + 4Be^{-2\\alpha} > 0$  \n",
    "Let$f^′(\\alpha) = 0$, we get $e^{\\alpha} = \\sqrt[3]{\\frac{2B}{A}}$  \n",
    "$min(Ae^{\\alpha} + Be^{-2\\alpha}) = \\sqrt[3]{{2B}{A^2}} + \\sqrt[3]{\\frac{A^2B}{4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) (vector calculus)  \n",
    "proof:  \n",
    "Let $\\frac{1}{2}w^TAw = r$, then $r$ contains $\\frac{1}{2}a_{11}w_1^2 + a_{12}w_1w_2+...+a_{1n}w_1w_n$, thus ${\\large \\frac{\\partial r}{\\partial w_1} }= \\sum_{i=1}^na_{1i}w_i$  \n",
    "Similarly for $w_k, 1<=k<=n.$ We have:  \n",
    "${\\large \\frac{\\partial r}{\\partial w_k} }= \\sum_{i=1}^na_{ki}w_i$, obviously ${\\large \\frac{\\partial (b^Tw)}{\\partial wk} }= \\sum_{i=1}^nb_i$, thus  \n",
    "$\\nabla E(w) = Aw + b$, also obviously  \n",
    "$\\nabla ^2E(w) = A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) (quadratic programming)  \n",
    "Since A is PD, $A = L^TDL$, where $D$ is diagonal matrix with all the positive eigen values and $L$ is invertible($A$ is invertible).  \n",
    "$\\frac{\\partial E}{\\partial w_k} = w^TL^TDL_{:,k} + b_k$  \n",
    "for every k, let:  \n",
    "$w^TL^TDL_{:,k} + b_k = 0$, we get:  \n",
    "$w^TL^TDL + b^T = 0$, $w = -A^{-1}b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) (optimization with linear constraints)  \n",
    "Let $L(\\lambda,w_1,w_2,w_3) = \\frac{1}{2}(w_1^2 + 2w_2^2 + 3w_3^2) - \\lambda(w_1+w_2+w_3-11)$  \n",
    "Let $\\frac{\\partial L}{\\partial w_1} = 0$, we get $w_1 - \\lambda = 0$  \n",
    "Let $\\frac{\\partial L}{\\partial w_2} = 0$, we get $w_2 - 2\\lambda = 0$  \n",
    "Let $\\frac{\\partial L}{\\partial w_3} = 0$, we get $w_3 - 3\\lambda = 0$  \n",
    "Let $\\frac{\\partial L}{\\partial \\lambda} = 0$, we get $w_1 + w_2 + w_3 = 11$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) (optimaization with linear constraints)  \n",
    "Assume $\\nabla E(w)$ does not satisfy $\\nabla E(w) +\\lambda^TA = 0$ when $w$ gives the min$E(w)$, let $u$ be the residual when projecting $\\nabla E(w)$ to the span of the rows of A, let $U$ be the residual when projecting $w$ to the span of the rows of A. For some very small $\\eta$, $w:= U - b + \\eta u$ is a feasible solution that improves $E$ beacause $Aw + b = 0$ still holds, contradiction! So the optimal solution must happen at $\\nabla E(w) +\\lambda^TA = 0$ for some vector $\\lambda$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
